{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33583d94-e35c-49bf-9bd9-cc4702b4573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import itertools\n",
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas.metrics import faithfulness, answer_relevancy, answer_correctness, context_precision, context_recall, answer_similarity\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17d23ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "# os.environ(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfb1a694-ee9b-493a-8dc2-6a5dcffc6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"../pdf/Mateo Wheeler Resume v2.pdf\"\n",
    "\n",
    "if local_path:\n",
    "    loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "    data = loader.load()\n",
    "else:\n",
    "    print(\"Upload a PDF file for processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de16b3c-6566-41d7-b78c-65a3a2167774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6828"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8b2137c-9ae8-445e-830f-4ca8ebb62f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "#Split and chunk the data\n",
    "chunk_size = 150\n",
    "chunk_overlap = 75\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "\n",
    "# Add the chunks to vector database, which uses nomic for model embeddings\n",
    "vector_db = Chroma.from_documents(\n",
    "                                    documents=chunks, \n",
    "                                    embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "                                    collection_name=\"local-rag\"\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e777e083-731e-4c6c-8e15-20455cd84035",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = \"llama3.2\"\n",
    "llm = ChatOllama(model=local_llm)\n",
    "\n",
    "# Set up a basic PromptTemplate as the backbones of the solution\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables = [\"question\"],\n",
    "        template=\"\"\"As a recruiter, you need to evaluate if a candidate is a good fit for a role. \n",
    "                If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "                Question: {question}\n",
    "           \"\"\"\n",
    ")\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(vector_db.as_retriever(),llm, prompt=QUERY_PROMPT)\n",
    "\n",
    "# use a ChatPromptTemplate to initiate a conversation, allowing the System to assume a Role\n",
    "chat_template = \"\"\"Answer the question based only on the following context: \n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(chat_template)\n",
    "\n",
    "chain = (\n",
    "    {\"context\":retriever, \"question\":RunnablePassthrough()}\n",
    "    | prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6917f39f-b4b3-4453-a07b-47d76e24bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy company_description and job_description from LinkedIn job posts\n",
    "company_description = \"\"\"Since its founding in 1993, NVIDIA (NASDAQ: NVDA) has been a pioneer in accelerated computing. The company’s invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined computer graphics, ignited the era of modern AI and is fueling the creation of the metaverse. NVIDIA is now a full-stack computing company with data-center-scale offerings that are reshaping industry.\n",
    "\"\"\"\n",
    "\n",
    "job_description = \"\"\"Professional, long-term experience as a Data Scientist (focused on AI/ML) developing/supporting business facing projects.\n",
    "8+ years of experience, including 5+ years of demonstrated ability in business-focused AI and Data Science. Job scope can be adjusted to accommodate more experienced candidates.\n",
    "BS/MS/PhD or equivalent experience in Computer Science, Data Science, Electrical/Computer Engineering, Physics, Mathematics, other Engineering fields. Technical Master’s or Ph.D. with finance or business background is preferred.\n",
    "Data Science project management, driving projects and coordinating across multidisciplinary teams inside the organization.\n",
    "Strong technical skills, with a proven history of designing, validating, deploying, and maintaining data science models using Python, SQL, & Databricks (or similar).\n",
    "Excellent communication skills, with the ability to maintain good documentation and present projects to technical and business partners.\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a19eaae-15d9-485b-bb05-3c3c509f3740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context of the resume document and the job requirements, I can provide an evaluation of the candidate's qualifications.\n",
      "\n",
      "The candidate has approximately 8 years of experience as a Senior Data Scientist at AirDNA and Hellofresh. They have worked on various projects, including developing Quasi-Experimental Evaluation Design (QED) for customer touchpoint hypothesis testing, creating data visualization dashboards, and managing the Data Science team. The candidate has also demonstrated expertise in machine learning, business analytics, and data visualization.\n",
      "\n",
      "The candidate's technical skills align well with the job requirements, as they have experience with Python, SQL, Databricks, Tableau, Power BI, and other relevant tools. They also have a strong background in statistics and programming languages such as PySpark, SAS, and NLTK.\n",
      "\n",
      "The candidate's experience in business-focused AI and data science is extensive, and they have a proven track record of driving projects and coordinating across multidisciplinary teams. Their excellent communication skills are evident in their ability to maintain good documentation and present projects to technical and business partners.\n",
      "\n",
      "However, the candidate lacks direct experience with NVIDIA-specific technologies or applications. Additionally, while they have relevant experience in data science and machine learning, there is no explicit mention of expertise in areas such as metaverse development or full-stack computing, which are essential for this role.\n",
      "\n",
      "Overall, I would rate the candidate's qualifications as follows:\n",
      "\n",
      "* Technical skills: 9/10 (aligned with job requirements)\n",
      "* Business-focused AI and data science experience: 8.5/10 (extensive experience, but some gaps in metaverse development or NVIDIA-specific technologies)\n",
      "* Project management and communication skills: 9/10 (excellent documentation and presentation skills)\n",
      "* Relevant certifications or education: 6/10 (no explicit mention of relevant certifications or degrees)\n",
      "\n",
      "Based on this evaluation, I would recommend further discussion with the candidate to address potential gaps in their experience and qualifications.\n"
     ]
    }
   ],
   "source": [
    "context = f\"Role: you are a recruiter for {company_description}. The job position requirements are the following:\\\n",
    "            {job_description}\\\n",
    "        \"\n",
    "\n",
    "q = \"Can you evaluate the candidate in this resume document for the role provided?\"\n",
    "\n",
    "response = chain.invoke(input={'context': context, 'question': q})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4b0d38",
   "metadata": {},
   "source": [
    "#### Evaluate LLM responses using Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e42b5a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a basic PromptTemplate as the backbones of the solution\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables = [\"question\"],\n",
    "        template=\"\"\"As a job recruiter, you want to understand a candidate's resume, but not specific to a job position. \n",
    "                If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "                Question: {question}\n",
    "           \"\"\"\n",
    ")\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(vector_db.as_retriever(),llm, prompt=QUERY_PROMPT)\n",
    "\n",
    "# use a ChatPromptTemplate to initiate a conversation, allowing the System to assume a Role\n",
    "chat_template = \"\"\"Answer the question based only on the following context: \n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(chat_template)\n",
    "\n",
    "chain = (\n",
    "    {\"context\":retriever, \"question\":RunnablePassthrough()}\n",
    "    | prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea7a0810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To calculate the total years of experience, let's break it down:\n",
      "\n",
      "1. PricewaterhouseCoopers (2010-2016): 6 years\n",
      "2. Master of Science in Statistics (2007-2009): 2 years\n",
      "\n",
      "Adding both together: 6 + 2 = 8 years\n",
      "\n",
      "So, this candidate has approximately 8 years of experience.\n"
     ]
    }
   ],
   "source": [
    "context0 = f\"Role: you are a generic recruiter reading a candidate's resume.\"\n",
    "q0 = \"Based on the dates listed for each position, how many years of experience does this candidate have?\"\n",
    "\n",
    "response0 = chain.invoke(input={'context': context0, 'question': q0})\n",
    "print(response0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d33f1c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The candidate is experienced with the following machine learning/libraries:\n",
      "\n",
      "1. SBERT\n",
      "2. NLTK\n",
      "3. scikit-learn\n",
      "4. UMAP\n",
      "5. PyTorch\n",
      "6. TensorFlow\n",
      "7. Nixtla TimeGPT\n",
      "8. Sagemaker DeepAR\n",
      "9. Catboost\n",
      "\n",
      "These libraries are used for tasks such as natural language processing, deep learning, reinforcement learning, and other machine learning-related tasks.\n"
     ]
    }
   ],
   "source": [
    "q1 = \"Based on this resume, what machine libraries is the candidate experienced with?\"\n",
    "\n",
    "response1 = chain.invoke(input={'context': context0, 'question': q1})\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85115a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"question\": [\n",
      "        \"Based on this resume, what machine libraries is the candidate experienced with?\"\n",
      "    ],\n",
      "    \"answer\": [\n",
      "        \"The candidate is experienced with the following machine learning/libraries:1. SBERT2. NLTK3. scikit-learn4. UMAP5. PyTorch6. TensorFlow7. Nixtla TimeGPT8. Sagemaker DeepAR9. CatboostThese libraries are used for tasks such as natural language processing, deep learning, reinforcement learning, and other machine learning-related tasks.\"\n",
      "    ],\n",
      "    \"contexts\": [\n",
      "        [\n",
      "            \"Role: you are a generic recruiter reading a candidate's resume.\"\n",
      "        ]\n",
      "    ],\n",
      "    \"ground_truth\": [\n",
      "        \"Tensorflow, PyTorch, scikit-learn, Prophet, Nixtla TimeGPT, Statsmodels, Sagemaker DeepAR, Catboost, sklearn, NLTK, Spacy, Word2Vec, SBERT/HuggingFace, OpenAI, Langchain, YOLO,  Deep learning, Reinforcement Learning, RAG Systems\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "data_samples = {\n",
    "    'question': [\n",
    "         #q0, \n",
    "         q1\n",
    "    ],\n",
    "    'answer': [\n",
    "         #response0.replace(\"\\n\", \"\"), \n",
    "         response1.replace(\"\\n\", \"\")\n",
    "\n",
    "    ],\n",
    "    'contexts': [\n",
    "        [\n",
    "            context0\n",
    "        ],\n",
    "        #[\n",
    "        #    context0\n",
    "        #]\n",
    "    ],\n",
    "    'ground_truth': [\n",
    "        #'The candidate has a total of 18.5 years of experience.',\n",
    "        'Tensorflow, PyTorch, scikit-learn, Prophet, Nixtla TimeGPT, Statsmodels, Sagemaker DeepAR, Catboost, sklearn, NLTK, Spacy, Word2Vec, SBERT/HuggingFace, OpenAI, Langchain, YOLO,  Deep learning, Reinforcement Learning, RAG Systems'\n",
    "    ]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "json_formatted_string = json.dumps(data_samples, indent=4)\n",
    "print(json_formatted_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d88668c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# llm_eval = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# attempt using llama embeddings as well\n",
    "# llm_llama3 = ChatOllama(model=\"llama3.2\",verbose=False,timeout=600,num_ctx=8192,disable_streaming=False, temperature=0)\n",
    "# embeddings_llama3 = OllamaEmbeddings(model=\"llama3.2\")\n",
    "\n",
    "# evaluator_llm = LangchainLLMWrapper(llm_llama3)\n",
    "# evaluator_embed = LangchainEmbeddingsWrapper(embeddings_llama3)\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "evaluator_embed = LangchainEmbeddingsWrapper(OllamaEmbeddings(model=\"nomic-embed-text\"))\n",
    "\n",
    "# Test if embeddings wrapper works\n",
    "try:\n",
    "    test_embedding = evaluator_embed.embed_query(\"test text\")\n",
    "    print(f\"Embedding dimension: {len(test_embedding)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Embeddings wrapper error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6261677d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98abeebac3a94a948d2031c645c28cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_precision': 0.0000, 'faithfulness': 0.0000, 'answer_relevancy': 0.8479, 'answer_correctness': 0.6410, 'semantic_similarity': 0.7861}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate using Ragas\n",
    "result = evaluate(\n",
    "    dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        #context_recall\n",
    "        answer_correctness,\n",
    "        answer_similarity,\n",
    "    ],\n",
    "    llm=evaluator_llm,\n",
    "    embeddings=evaluator_embed,\n",
    "    run_config=RunConfig(timeout=300, \n",
    "                         max_retries=3, # Reduce retries initially\n",
    "                         max_wait=300, \n",
    "                         log_tenacity=True # Enable logging for debugging\n",
    "    ),\n",
    "    raise_exceptions=False # Don't stop on first error\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33424c09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
